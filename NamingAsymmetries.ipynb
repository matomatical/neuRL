{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naming asymmetries\n",
    "\n",
    "$\\epsilon$s are 'expectiles', what is a good word for describing the $\\tau$s?\n",
    "\n",
    "Whatever the choice, it should recover the following meanings:\n",
    "\n",
    "* $\\tau=0.5$ implies a 'balanced' expectile,\n",
    "* $\\tau > 0.5$ implies an 'optimistic' expectile,\n",
    "* $\\tau < 0.5$ implies a 'pessimistic' expectile.\n",
    "\n",
    "## The word\n",
    "\n",
    "The deepmind paper calls $\\tau$ an 'asymetric scaling', or 'asymetric scaling ratio', referring to $\\tau = \\alpha_+ / (\\alpha_+ + \\alpha_-)$, where $\\alpha$s are 'scalings' (learning rates). They refer to a 'diversity of expectiles' rather than a diversity in expectile-parameters. In the supplemental information, they refer to the $\\tau$s as 'asymmetries' (and sometimes use the symbol $\\eta$ rather than $\\tau$).\n",
    "\n",
    "How about a term from psychology, such as 'attitude', 'disposition', 'outlook', 'valence'? We can talk about 'distribution of attitudes', for example.\n",
    "\n",
    "## The scaling\n",
    "\n",
    "We need not use $\\tau \\in [0, 1]$ itself, since for example the 0.5 centering may be counter-intuitive.\n",
    "\n",
    "For example, we could subtract 0.5 to center a quantity at 0, so that 0 corresponds to a 'balanced attitude', positive numbers correspond to 'positive attitude', negative numbers correspond to 'negative attitude'. The measure would range from $[-0.5, 0.5]$.\n",
    "\n",
    "We could also talk about the ratio $\\tau / (1-\\tau) \\in [0, \\infty)$ instead of $\\tau$ itself, since this quantity is used in actually setting the learning rates. This is centered (for $\\tau=0.5$) at 1.\n",
    "\n",
    "We could further talk about the *logarithm* of this ratio, for a number in $\\mathbb{R}$, also centered a 0 (so 'positive' and 'negative' terminology works).\n",
    "\n",
    "If we call this quantity $\\theta$ then the following relationships hold:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta\n",
    "&= \\text{logit}(\\tau) = \\log\\left(\\frac{\\tau}{1-\\tau}\\right)\n",
    "&\n",
    "\\tau\n",
    "&= \\text{logistic}(\\theta) = \\sigma(\\theta) = \\frac{1}{1+\\exp(-\\theta)}\n",
    "\\\\\n",
    "\\alpha_+\n",
    "&= \\alpha \\cdot \\sqrt{\\frac{\\tau}{1-\\tau}}\n",
    "=  \\exp\\left(\\frac{1}{2}\\theta\\right)\n",
    "&\n",
    "\\alpha_-\n",
    "&= \\alpha \\cdot \\sqrt{\\frac{1-\\tau}{\\tau}}\n",
    "=  \\exp\\left(-\\frac{1}{2}\\theta\\right)\n",
    "\\end{align*}\n",
    "        \n",
    "## The symbol\n",
    "\n",
    "I can't remember where I got tau from (maybe one of the expectile papers? or deepmind?).\n",
    "\n",
    "For the logit version, there's no clear choice for a symbol. If we go with 'attitude', the greek word is 'στάση' (stance) or 'διάθεση' (disposition). Neither greek letter is suitable because sigma conflicts with the sigmoid function and delta is used to denote the reward prediction error itself. Neither is alpha from 'αισιοδοξία' (optimism) and 'απαισιοδοξία' (pessimism) suitable, due to the conflict with the learning rate.\n",
    "\n",
    "Are we stuck with theta with no satisfying reason...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
