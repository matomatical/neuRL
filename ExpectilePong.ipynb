{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:      3.8.2\n",
      "NumPy version:       1.18.1\n",
      "TensorFlow version:  2.2.0-rc4\n",
      " Eager execution:     True\n",
      " GPU availability:    False\n",
      "Sonnet version:      2.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import sonnet as snt\n",
    "import tqdm.notebook as tqdm\n",
    "import gym\n",
    "\n",
    "from util.expectiles import expectile\n",
    "\n",
    "print(f\"Python version:      {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version:       {np.__version__}\")\n",
    "print(f\"TensorFlow version:  {tf.__version__}\")\n",
    "print(f\" Eager execution:     {tf.executing_eagerly()}\")\n",
    "print(f\" GPU availability:    {bool(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "print(f\"Sonnet version:      {snt.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode finished after 1120 timesteps\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "for t in itertools.count(1):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done: break\n",
    "print(f\"episode finished after {t} timesteps\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionalActorCritic(snt.Module):\n",
    "    def __init__(self, taus, n_actions):\n",
    "        super(DistributionalActorCritic, self).__init__()\n",
    "        self.norm = tf.constant(255, dtype=tf.float32)\n",
    "        self.cortex = snt.Sequential([\n",
    "            snt.Conv2D(16, 3, 1), tf.nn.relu, # conv layer 1\n",
    "            snt.Conv2D(16, 3, 1), tf.nn.relu, # conv layer 2\n",
    "            snt.Flatten(),\n",
    "            snt.Linear(32), tf.nn.relu,        # fully-connected layer 1\n",
    "            snt.Linear(16), tf.nn.relu,        # output (to actor/critic)\n",
    "        ])\n",
    "        self.critic = snt.Linear(taus.size)\n",
    "        self.actor  = snt.Linear(n_actions)\n",
    "        self.tau_factors = tf.convert_to_tensor(np.sqrt(taus/(1-taus)), dtype=tf.float32)\n",
    "    def __call__(self, states):\n",
    "        represn = self.cortex(states/self.norm)\n",
    "        values = self.critic(represn)\n",
    "        action_logits = self.actor(represn)\n",
    "        print(len(action_logits[0]))\n",
    "        print(states.shape[0])\n",
    "        action = tf.random.categorical(action_logits, states.shape[0])[:, 0]\n",
    "#         print(action.numpy())\n",
    "        return (values, action)\n",
    "    def loss(self, value_predictions, targets):\n",
    "        rpes = tf.subtract(targets[:, tf.newaxis], value_predictions)\n",
    "        loss = tf.reduce_mean(tf.pow(self.tau_factors, tf.sign(rpes)) * tf.square(rpes))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributionalActorCritic(taus=array([0.5]), n_actions=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.13548037]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistributionalActorCritic(np.array([0.5]), env.action_space.n)\n",
    "print(model)\n",
    "model(state[tf.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training!\n",
      "step 1 last action tf.Tensor([6], shape=(1,), dtype=int64)\r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-fbf4474a4216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mvx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# WHERE TF ARE THESE NANS COMING FROM?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "print(\"Begin training!\")\n",
    "done = False\n",
    "x = env.reset()\n",
    "env.render()\n",
    "optimizer = snt.optimizers.SGD(learning_rate=0.00001)\n",
    "for t in itertools.count(1):\n",
    "    print('step', t, 'last action', a, end=\"\\r\")\n",
    "    with tf.GradientTape() as tx:\n",
    "        vx, a = model(x[tf.newaxis])\n",
    "        # WHERE TF ARE THESE NANS COMING FROM?\n",
    "    y, r, done, _ = env.step(a[0])\n",
    "    env.render()\n",
    "    r = tf.convert_to_tensor(r, dtype=tf.float32)\n",
    "    vy, _ = model(y[tf.newaxis])\n",
    "    with tx:\n",
    "        loss = model.loss(vx, r+vy)\n",
    "    # do gradient update!\n",
    "    variables = model.trainable_variables\n",
    "    gradients = tx.gradient(loss, variables)\n",
    "    optimizer.apply(gradients, variables)\n",
    "    if done: break\n",
    "print(f\"episode finished after {t} timesteps\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, labels_rewards, loss_fn, data=TRAIN_DATA,\n",
    "#           num_items=NUM_REDUCED_IMAGES, num_epochs=NUM_EPOCHS,\n",
    "#           learning_rate=LEARNING_RATE, **loss_kwargs):\n",
    "#     progress = tqdm.tqdm(total=num_items*num_epochs, unit=\"images\")\n",
    "#     loss_log = []\n",
    "#     loss_log_2 = []\n",
    "#     optimizer = snt.optimizers.SGD(learning_rate=learning_rate)\n",
    "#     for minibatch in data.repeat(num_epochs):\n",
    "#         images, labels = minibatch\n",
    "#         # generate rewards for batch\n",
    "#         rewards = np.zeros(labels.shape[0])\n",
    "#         for label, rs in labels_rewards.items():\n",
    "#             ids = np.where(labels == label)\n",
    "#             num = ids[0].size\n",
    "#             rewards[ids] = np.random.choice(rs, num)\n",
    "#         rewards = tf.convert_to_tensor(rewards, tf.float32)\n",
    "#         # predict rewards, compute loss\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             value_predictions = model(images)\n",
    "#             loss = loss_fn(value_predictions, rewards, **loss_kwargs)\n",
    "#         # apply update\n",
    "#         variables = model.trainable_variables\n",
    "#         gradients = tape.gradient(loss, variables)\n",
    "#         optimizer.apply(gradients, variables)\n",
    "#         # track progress\n",
    "#         print(f\"loss {loss.numpy():15.3f}\", end=\"\\r\")\n",
    "#         loss_log.append(loss.numpy())\n",
    "#         progress.update(n=labels.shape[0])\n",
    "#     print(f\"loss {loss.numpy():15.3f} (done)\")\n",
    "#     progress.close()\n",
    "#     return model, loss_log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
