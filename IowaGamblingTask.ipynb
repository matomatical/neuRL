{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iowa Gambling Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheme 1\n",
    "\n",
    "> Rewards and losses from 40 choices of each deck as used in the traditional payoff scheme with variable loss in deck C (see Bechara et al.[1]; classified here as payoff scheme 1). Within each deck, the presented payoff sequence is repeated after participants have made 40 choices from the corresponding deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardScheme1:\n",
    "    def __init__(self, random_seed=None):\n",
    "        self.acounts = np.zeros(4, dtype=int)\n",
    "        self.pos = np.array([100, 100, 50, 50,])\n",
    "        self.neg = np.array([\n",
    "            [    0,     0,  -150,     0,  -300,     0,  -200,     0,  -250,  -350,\n",
    "                 0,  -350,     0,  -250,  -200,     0,  -300,  -150,     0,     0,\n",
    "                 0,  -300,     0,  -350,     0,  -200,  -250,  -150,     0,     0,\n",
    "              -350,  -200,  -250,     0,     0,     0,  -150,  -300,     0,     0,\n",
    "            ],\n",
    "            [    0,     0,     0,     0,     0,     0,     0,     0, -1250,     0,\n",
    "                 0,     0,     0, -1250,     0,     0,     0,     0,     0,     0,\n",
    "             -1250,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "                 0, -1250,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            ],\n",
    "            [    0,     0,   -50,     0,   -50,     0,   -50,     0,   -50,   -50,\n",
    "                 0,   -25,   -75,     0,     0,     0,   -25,   -75,     0,   -50,\n",
    "                 0,     0,     0,   -50,   -25,   -50,     0,     0,   -75,   -50,\n",
    "                 0,     0,     0,   -25,   -25,     0,   -75,     0,   -50,   -75,\n",
    "            ],\n",
    "            [    0,     0,     0,     0,     0,     0,     0,     0,     0,  -250,\n",
    "                 0,     0,     0,     0,     0,     0,     0,     0,     0,  -250,\n",
    "                 0,     0,     0,     0,     0,     0,     0,     0,  -250,     0,\n",
    "                 0,     0,     0,     0,  -250,     0,     0,     0,     0,     0\n",
    "            ],\n",
    "        ])\n",
    "    def reset(self):\n",
    "        self.acounts = np.zeros(4, dtype=int)\n",
    "    def rewards(self, action):\n",
    "        a = \"ABCD\".index(action)\n",
    "        r = (self.pos[a], self.neg[a, self.acounts[a] % 40])\n",
    "        self.acounts[a] += 1\n",
    "        return r\n",
    "    def __str__(self):\n",
    "        return \"Scheme 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheme 2\n",
    "\n",
    "> A possible sequence of rewards and losses from 10 choices of each deck based on the traditional payoff scheme with constant loss in deck C (see Bechara et al.[1]; classified here as payoff scheme 2). A payoff sequence with the presented characteristics is randomly generated for each block of 10 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardScheme2:\n",
    "    def __init__(self, random_seed=None):\n",
    "        self.n = 0\n",
    "        self.pos = np.array([100, 100, 50, 50,])\n",
    "        self.neg = np.array([\n",
    "            [    0,  -300,  -150,     0,  -350,     0,     0,  -250,     0,  -200],\n",
    "            [    0,     0,     0,     0,     0, -1250,     0,     0,     0,     0],\n",
    "            [    0,     0,   -50,     0,   -50,     0,   -50,     0,   -50,   -50],\n",
    "            [    0,     0,     0,     0,     0,     0,     0,     0,  -250,     0],\n",
    "        ])\n",
    "        self.rng = np.random.default_rng(seed=random_seed)\n",
    "    def reset(self, random_state=None):\n",
    "        if random_state: self.rng.set_state(random_state)\n",
    "        self.n = 0\n",
    "    def rewards(self, action):\n",
    "        # reorder the schemes every 10 rounds\n",
    "        if self.n % 10 == 0:\n",
    "            for a in range(4):\n",
    "                self.rng.shuffle(self.neg[a])\n",
    "        self.n += 1\n",
    "        # draw a reward from the scheme\n",
    "        a = \"ABCD\".index(action)\n",
    "        r = (self.pos[a], self.neg[a, self.n % 10])\n",
    "        return r\n",
    "    def __str__(self):\n",
    "        return \"Scheme 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheme 3\n",
    "\n",
    "> Rewards and losses from 60 choices of each deck as used in the payoff scheme introduced by Bechara & Damasio ([2]; classified here as payoff scheme 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheme R\n",
    "\n",
    "> Table 4: Schemes of Iowa Gambling Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardSchemeR:\n",
    "    def __init__(self, random_seed=None, variable_C=True):\n",
    "        self.pos = np.array([100, 100, 50, 50])\n",
    "        self.neg = np.array([\n",
    "            [0, -150, -200, -250, -300, -350],\n",
    "            [0, -1250],\n",
    "            [0, -25, -50, -75] if variable_C else [0, -50],\n",
    "            [0, -250],\n",
    "        ])\n",
    "        self.prs = np.array([\n",
    "            [0.5, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "            [0.9, 0.1],\n",
    "            [0.5, 0.1, 0.3, 0.1] if variable_C else [0.5, 0.5],\n",
    "            [0.9, 0.1],\n",
    "        ])\n",
    "        self.rng = np.random.default_rng(seed=random_seed)\n",
    "    def reset(self):\n",
    "        pass\n",
    "    def rewards(self, action):\n",
    "        a = \"ABCD\".index(action)\n",
    "        return (self.pos[a], self.rng.choice(self.neg[a], p=self.prs[a]))\n",
    "    def __str__(self):\n",
    "        return \"Scheme R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IowaGamblingTask:\n",
    "    def __init__(self, scheme=\"R\", max_nrounds=100, split_rewards=False, random_seed=None):\n",
    "        self.max_nrounds = max_nrounds\n",
    "        if scheme == \"1\" or scheme == 1:\n",
    "            self.scheme = RewardScheme1()\n",
    "        elif scheme == \"2\" or scheme == 2:\n",
    "            self.scheme = RewardScheme2(random_seed=random_seed)\n",
    "        elif scheme == \"3\" or scheme == 3:\n",
    "            raise Exception(\"Sorry, scheme 3 implementation missing.\")\n",
    "        elif scheme == \"R\":\n",
    "            self.scheme = RewardSchemeR(random_seed=random_seed)\n",
    "        else:\n",
    "            self.scheme = scheme # better be a reward scheme!\n",
    "        self.split_rewards = split_rewards\n",
    "        self.action_space = np.arange(4)\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.scheme.reset()\n",
    "        self.nrounds = 0\n",
    "        return self.nrounds\n",
    "    def step(self, action):\n",
    "        self.nrounds += 1\n",
    "        reward = self.scheme.rewards(\"ABCD\"[action])\n",
    "        if not self.split_rewards:\n",
    "            reward = sum(reward)\n",
    "        done = (self.nrounds == self.max_nrounds)\n",
    "        return self.nrounds, reward, done, {}\n",
    "    def render(self, end=\"\\r\", **kwargs):\n",
    "        print(\"Iowa Gambling Task ({}).\".format(self.scheme), \n",
    "              \"{:3d} / {:3d} rounds complete.\".format(self.nrounds, self.max_nrounds),\n",
    "              end=end, **kwargs)\n",
    "    def close(self, **kwargs):\n",
    "        print(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iowa Gambling Task (Scheme R). 100 / 100 rounds complete.\n"
     ]
    }
   ],
   "source": [
    "env = IowaGamblingTask(split_rewards=True, scheme=\"R\")\n",
    "done = False\n",
    "s = env.reset()\n",
    "env.render()\n",
    "while not done:\n",
    "    s, r, done, _ = env.step(0)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStepIowaGamblingTask:\n",
    "    def __init__(self, scheme=\"R\", max_nrounds=100, split_rewards=False, random_seed=None):\n",
    "        self.max_nrounds = max_nrounds\n",
    "        if scheme == \"1\" or scheme == 1:\n",
    "            self.scheme = RewardScheme1()\n",
    "        elif scheme == \"2\" or scheme == 2:\n",
    "            self.scheme = RewardScheme2(random_seed=random_seed)\n",
    "        elif scheme == \"3\" or scheme == 3:\n",
    "            raise Exception(\"Sorry, scheme 3 implementation missing.\")\n",
    "        elif scheme == \"R\":\n",
    "            self.scheme = RewardSchemeR(random_seed=random_seed)\n",
    "        else:\n",
    "            self.scheme = scheme # better be a reward scheme!\n",
    "        self.split_rewards = split_rewards\n",
    "        self.action_spaces = [np.arange(4), np.arange(1), np.arange(1), np.arange(1), np.arange(1)]\n",
    "        self.state = 0\n",
    "    def reset(self):\n",
    "        self.scheme.reset()\n",
    "        self.nrounds = 0\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    def step(self, action):\n",
    "        if self.state == 0:\n",
    "            self.state = action+1\n",
    "            return self.state, 0, False, {}\n",
    "        else:\n",
    "            # ignore action and sample reward\n",
    "            reward = self.scheme.rewards(\"ABCD\"[self.state-1])\n",
    "            if not self.split_rewards:\n",
    "                reward = sum(reward)\n",
    "            self.nrounds += 1\n",
    "            done = (self.nrounds == self.max_nrounds)\n",
    "            self.state = 0\n",
    "            return 0, reward, done, {}\n",
    "    def render(self, end=\"\\r\", **kwargs):\n",
    "        print(\"Two-step Iowa Gambling Task ({}).\".format(self.scheme), \n",
    "              \"{:3d} / {:3d} rounds complete.\".format(self.nrounds, self.max_nrounds),\n",
    "              end=end, **kwargs)\n",
    "    def close(self, **kwargs):\n",
    "        print(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-step Iowa Gambling Task (Scheme R). 100 / 100 rounds complete.\n"
     ]
    }
   ],
   "source": [
    "env = TwoStepIowaGamblingTask(split_rewards=True, scheme=\"R\")\n",
    "done = False\n",
    "s = env.reset()\n",
    "env.render()\n",
    "while not done:\n",
    "    s, r, done, _ = env.step(0)\n",
    "    env.render()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
